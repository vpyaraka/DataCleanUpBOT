# -*- coding: utf-8 -*-
"""DataCleansingBOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bh7hbBRCkBz5C9mUXR_lAPuTN0Y3Mc3I
"""

# streamlit_data_cleansing_bot.py
# Streamlit BOT: Upload CSV/Excel, Clean Data, Generate Rule‚ÄëBased Narrative Insights with Charts,
# and export Cleaned CSV/XLSX + a configurable PDF report.
#
# How to run:
#   pip install streamlit pandas numpy matplotlib seaborn fpdf xlsxwriter
#   streamlit run streamlit_data_cleansing_bot.py

import streamlit as st
import pandas as pd
import numpy as np
import io
import os
import tempfile
import matplotlib.pyplot as plt
import seaborn as sns
from fpdf import FPDF

st.set_page_config(page_title="Data Cleansing & Insights BOT", layout="wide")
st.title("ü§ñ Data Cleansing & Insights BOT")
st.write(
    "Upload a CSV/Excel file to clean data, view insights & charts, and download a cleaned dataset and/or a PDF report."
)

# ============================
# Sidebar Controls
# ============================
st.sidebar.header("‚öôÔ∏è Settings")
missing_numeric_strategy = st.sidebar.selectbox(
    "Numeric missing values",
    ["Mean", "Median", "Zero", "Drop rows"],
    index=0,
)
missing_categorical_strategy = st.sidebar.selectbox(
    "Categorical missing values",
    ["Mode", "Constant <missing>", "Drop rows"],
    index=0,
)

st.sidebar.markdown("---")
st.sidebar.header("üßæ PDF Report Sections")
include_summary = st.sidebar.checkbox("Narrative summary", value=True)
include_missing_table = st.sidebar.checkbox("Missing values table", value=True)
include_describe_table = st.sidebar.checkbox("Summary statistics table", value=True)
include_heatmap = st.sidebar.checkbox("Correlation heatmap", value=True)
include_cat_dists = st.sidebar.checkbox("Top categorical distributions", value=True)
include_num_dists = st.sidebar.checkbox("Numeric distributions", value=True)

# ============================
# File Upload
# ============================
uploaded_file = st.file_uploader("Upload CSV or Excel file", type=["csv", "xlsx"])

if uploaded_file:
    # Read file
    if uploaded_file.name.lower().endswith(".csv"):
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.read_excel(uploaded_file)

    st.subheader("üìÇ Raw Data Preview")
    st.dataframe(df.head())

    # ============================
    # Data Cleansing
    # ============================
    st.subheader("üßπ Data Cleansing")

    # 1) Standardize column names
    df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]

    # 2) Remove duplicates
    dupes_before = df.duplicated().sum()
    df = df.drop_duplicates()

    # 3) Replace common missing markers with NaN
    df = df.replace(["?", "", "NA", "na", "N/A", "n/a", "None", "none"], np.nan)

    # 4) Fix dtypes where numbers are stored as strings
    for col in df.columns:
        if df[col].dtype == object:
            # try numeric conversion gracefully
            coerced = pd.to_numeric(df[col], errors="ignore")
            df[col] = coerced

    # 5) Handle missing values
    # Numeric
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()

    if missing_numeric_strategy == "Mean":
        for c in num_cols:
            df[c] = df[c].fillna(df[c].mean())
    elif missing_numeric_strategy == "Median":
        for c in num_cols:
            df[c] = df[c].fillna(df[c].median())
    elif missing_numeric_strategy == "Zero":
        for c in num_cols:
            df[c] = df[c].fillna(0)
    elif missing_numeric_strategy == "Drop rows":
        df = df.dropna(subset=num_cols)

    # Categorical
    if missing_categorical_strategy == "Mode":
        for c in cat_cols:
            if df[c].isnull().sum() > 0:
                try:
                    df[c] = df[c].fillna(df[c].mode().iloc[0])
                except Exception:
                    df[c] = df[c].fillna("<missing>")
    elif missing_categorical_strategy == "Constant <missing>":
        for c in cat_cols:
            df[c] = df[c].fillna("<missing>")
    elif missing_categorical_strategy == "Drop rows":
        df = df.dropna(subset=cat_cols)

    st.success(
        f"Data cleaned ‚úÖ | Duplicates removed: {dupes_before} | Rows: {df.shape[0]} | Cols: {df.shape[1]}"
    )

    # ============================
    # Insights & Charts
    # ============================
    st.subheader("üìä Useful Insights")

    st.write("### üîç Summary Statistics (table)")
    describe_df = df.describe(include="all", datetime_is_numeric=True)
    st.dataframe(describe_df)

    st.write("### ‚ùì Missing Values per Column")
    missing_series = df.isnull().sum()
    st.dataframe(missing_series.to_frame("missing_count"))

    # Correlation Heatmap
    st.write("### üìà Correlation Heatmap")
    corr = df.corr(numeric_only=True)
    heatmap_path = None
    if not corr.empty:
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", ax=ax)
        st.pyplot(fig)
        tmp_heatmap = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
        fig.savefig(tmp_heatmap.name, bbox_inches="tight")
        heatmap_path = tmp_heatmap.name
    else:
        st.info("No numeric columns available for correlation analysis.")

    # Categorical Distributions (Top 10)
    st.write("### üè∑Ô∏è Top Categorical Distributions")
    cat_plots = []
    for col in cat_cols:
        st.write(f"**{col}**")
        st.write(df[col].value_counts().head(10))
        fig, ax = plt.subplots()
        df[col].value_counts().head(10).plot(kind="bar", ax=ax)
        ax.set_title(f"Top categories in {col}")
        ax.set_xlabel(col)
        ax.set_ylabel("count")
        plt.xticks(rotation=45, ha="right")
        st.pyplot(fig)
        tmp_img = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
        fig.savefig(tmp_img.name, bbox_inches="tight")
        cat_plots.append(tmp_img.name)

    # Numeric Distributions
    st.write("### üìä Numeric Distributions")
    num_plots = []
    for col in num_cols:
        fig, ax = plt.subplots()
        sns.histplot(df[col], kde=True, ax=ax)
        ax.set_title(f"Distribution of {col}")
        ax.set_xlabel(col)
        st.pyplot(fig)
        tmp_img = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
        fig.savefig(tmp_img.name, bbox_inches="tight")
        num_plots.append(tmp_img.name)

    # ============================
    # Rule‚ÄëBased Narrative Summary
    # ============================
    def rule_based_summary(df: pd.DataFrame, corr_df: pd.DataFrame) -> str:
        lines = []
        # Size
        lines.append(
            f"The dataset contains {df.shape[0]:,} rows and {df.shape[1]:,} columns."
        )
        # Missing
        miss = df.isnull().sum().sort_values(ascending=False)
        if (miss > 0).any():
            top_missing = miss[miss > 0].head(5)
            parts = [f"{idx} ({val})" for idx, val in top_missing.items()]
            lines.append(
                "Missing values are present, concentrated in: " + ", ".join(parts) + "."
            )
        else:
            lines.append("No missing values detected after cleaning.")
        # Categorical
        cat_cols_local = df.select_dtypes(exclude=[np.number]).columns.tolist()
        if cat_cols_local:
            for c in cat_cols_local[:3]:
                vc = df[c].value_counts(dropna=False)
                if not vc.empty:
                    top_label = vc.index[0]
                    top_pct = (vc.iloc[0] / len(df)) * 100
                    lines.append(
                        f"Column '{c}' is dominated by '{top_label}' (~{top_pct:.1f}%)."
                    )
        # Numeric
        num_cols_local = df.select_dtypes(include=[np.number]).columns.tolist()
        for c in num_cols_local[:3]:
            series = df[c].dropna()
            if len(series) > 0:
                lines.append(
                    f"'{c}' ranges {series.min():.2f}‚Äì{series.max():.2f}, median {series.median():.2f}."
                )
        # Correlations
        if corr_df is not None and not corr_df.empty:
            # get top absolute correlations excluding self
            corr_unstack = (
                corr_df.where(~np.eye(len(corr_df), dtype=bool)).abs().unstack()
            )
            corr_unstack = corr_unstack.dropna().sort_values(ascending=False)
            seen = set()
            top_pairs = []
            for (a, b), val in corr_unstack.items():
                key = tuple(sorted([a, b]))
                if a == b or key in seen:
                    continue
                seen.add(key)
                top_pairs.append((a, b, val))
                if len(top_pairs) >= 3:
                    break
            if top_pairs:
                parts = [f"{a} ‚Üî {b} ({v:.2f})" for a, b, v in top_pairs]
                lines.append("Top correlations: " + ", ".join(parts) + ".")
        return " " .join(lines)

    narrative_text = rule_based_summary(df, corr)

    st.subheader("üß† Narrative Summary (Rule‚Äëbased)")
    st.write(narrative_text)

    # ============================
    # Downloads: Cleaned CSV/XLSX
    # ============================
    st.subheader("‚¨áÔ∏è Download Cleaned Data")
    csv_bytes = df.to_csv(index=False).encode("utf-8")
    st.download_button("Download Cleaned CSV", csv_bytes, "cleaned_data.csv", "text/csv")

    xlsx_buffer = io.BytesIO()
    with pd.ExcelWriter(xlsx_buffer, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="CleanedData")
    st.download_button(
        label="Download Cleaned Excel",
        data=xlsx_buffer,
        file_name="cleaned_data.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )

    # ============================
    # PDF Report Generation (Configurable)
    # ============================
    st.subheader("üìë Generate PDF Report")

    def generate_pdf_report(
        df: pd.DataFrame,
        describe_df: pd.DataFrame,
        missing_series: pd.Series,
        narrative: str,
        heatmap_path: str | None,
        cat_plots: list[str],
        num_plots: list[str],
        sections: dict,
    ) -> str:
        pdf = FPDF()
        pdf.set_auto_page_break(auto=True, margin=12)

        # Title Page
        pdf.add_page()
        pdf.set_font("Arial", "B", 18)
        pdf.cell(0, 10, "Data Cleansing & Insights Report", ln=True, align="C")
        pdf.ln(4)
        pdf.set_font("Arial", size=12)
        pdf.cell(0, 8, f"Rows: {df.shape[0]:,} | Columns: {df.shape[1]:,}", ln=True, align="C")

        # Narrative Summary
        if sections.get("summary") and narrative:
            pdf.add_page()
            pdf.set_font("Arial", "B", 14)
            pdf.cell(0, 8, "Narrative Summary", ln=True)
            pdf.set_font("Arial", size=12)
            # Split narrative into manageable lines
            for para in narrative.split(". "):
                pdf.multi_cell(0, 7, para.strip())
                pdf.ln(1)

        # Missing Table (first 40 rows)
        if sections.get("missing_table"):
            pdf.add_page()
            pdf.set_font("Arial", "B", 14)
            pdf.cell(0, 8, "Missing Values per Column", ln=True)
            pdf.set_font("Arial", size=11)
            for idx, val in missing_series.items():
                pdf.cell(0, 6, f"{idx}: {int(val)}", ln=True)

        # Describe Table (trimmed)
        if sections.get("describe_table"):
            pdf.add_page()
            pdf.set_font("Arial", "B", 14)
            pdf.cell(0, 8, "Summary Statistics", ln=True)
            pdf.set_font("Arial", size=9)
            # Render a subset for readability
            trimmed = describe_df.copy()
            trimmed = trimmed.fillna("")
            # Convert to rows of strings
            rows = [
                ["stat"] + [str(c) for c in trimmed.columns.tolist()]
            ]
            for idx, row in trimmed.iterrows():
                rows.append([str(idx)] + [str(x) for x in row.tolist()])
            # Print as text table (simple)
            for r in rows[:40]:
                pdf.multi_cell(0, 5, " | ".join(r))

        # Heatmap
        if sections.get("heatmap") and heatmap_path:
            pdf.add_page()
            pdf.set_font("Arial", "B", 14)
            pdf.cell(0, 8, "Correlation Heatmap", ln=True)
            pdf.image(heatmap_path, w=190)

        # Categorical plots
        if sections.get("cat_dists") and cat_plots:
            for img in cat_plots:
                pdf.add_page()
                pdf.set_font("Arial", "B", 14)
                pdf.cell(0, 8, "Categorical Distribution", ln=True)
                pdf.image(img, w=190)

        # Numeric plots
        if sections.get("num_dists") and num_plots:
            for img in num_plots:
                pdf.add_page()
                pdf.set_font("Arial", "B", 14)
                pdf.cell(0, 8, "Numeric Distribution", ln=True)
                pdf.image(img, w=190)

        tmp_pdf = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
        pdf.output(tmp_pdf.name)
        return tmp_pdf.name

    sections = {
        "summary": include_summary,
        "missing_table": include_missing_table,
        "describe_table": include_describe_table,
        "heatmap": include_heatmap,
        "cat_dists": include_cat_dists,
        "num_dists": include_num_dists,
    }

    if st.button("Generate PDF Report"):
        pdf_path = generate_pdf_report(
            df=df,
            describe_df=describe_df,
            missing_series=missing_series,
            narrative=narrative_text,
            heatmap_path=heatmap_path,
            cat_plots=cat_plots,
            num_plots=num_plots,
            sections=sections,
        )
        with open(pdf_path, "rb") as f:
            st.download_button(
                "‚¨áÔ∏è Download PDF Report",
                f,
                file_name="data_insights_report.pdf",
                mime="application/pdf",
            )